import os
import gradio as gr
import requests
import json
import time
import base64
import tempfile
import shutil
import threading
import uuid
from pathlib import Path

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è API –∫–ª—é—á–∞
api_key = os.environ.get("GLAMA_API_KEY")
if not api_key:
    raise ValueError("API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è GLAMA_API_KEY.")

# –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ñ–∞–π–ª–æ–≤
TEMP_DIR = Path(tempfile.gettempdir()) / "glama_chat_files"
TEMP_DIR.mkdir(exist_ok=True)

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (10 –ú–ë)
MAX_FILE_SIZE = 10 * 1024 * 1024

# –ö—ç—à –¥–ª—è –º–æ–¥–µ–ª–µ–π
models_cache = {
    "timestamp": 0,
    "models": ["gpt-4.5-preview-2025-02-27", "openai/gpt-4.5-preview-2025-02-27", "anthropic/claude-3-opus-20240229"]
}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
def get_available_models(force_refresh=False):
    current_time = time.time()
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à, –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å—Ç–∞—Ä—à–µ 1 —á–∞—Å–∞ –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
    if not force_refresh and current_time - models_cache["timestamp"] < 3600:
        return models_cache["models"]
    
    try:
        # –ó–∞–ø—Ä–æ—Å –∫ API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π
        response = requests.get(
            "https://glama.ai/api/gateway/openai/v1/models",
            headers={"Authorization": f"Bearer {api_key}"},
            timeout=10  # –¢–∞–π–º–∞—É—Ç 10 —Å–µ–∫—É–Ω–¥
        )
        
        if response.status_code == 200:
            models_data = response.json()
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ ID –º–æ–¥–µ–ª–µ–π –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Ö
            model_ids = [model["id"] for model in models_data["data"]]
            model_ids.sort()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à
            models_cache["timestamp"] = current_time
            models_cache["models"] = model_ids
            
            return model_ids
        else:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {response.status_code}")
            return models_cache["models"]
    except Exception as e:
        print(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {str(e)}")
        return models_cache["models"]

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤
def process_files(files):
    file_contents = []
    file_paths = []
    
    for file in files:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        if os.path.getsize(file.name) > MAX_FILE_SIZE:
            file_contents.append(f"–§–∞–π–ª {file.name} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å–∏–º—É–º 10 –ú–ë).")
            continue
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        file_ext = os.path.splitext(file.name)[1]
        temp_file = TEMP_DIR / f"{uuid.uuid4()}{file_ext}"
        
        # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        shutil.copy2(file.name, temp_file)
        file_paths.append(temp_file)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        file_type = "image" if file.name.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.webp')) else "file"
        
        if file_type == "image":
            # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ —Ç–µ–∫—Å—Ç
            pass
        else:
            # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ —á–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            try:
                with open(file.name, 'r', encoding='utf-8') as f:
                    file_content = f.read()
                    file_contents.append(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ {os.path.basename(file.name)}:\n{file_content}")
            except UnicodeDecodeError:
                try:
                    # –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É
                    with open(file.name, 'r', encoding='latin-1') as f:
                        file_content = f.read()
                        file_contents.append(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ {os.path.basename(file.name)}:\n{file_content}")
                except:
                    file_contents.append(f"–§–∞–π–ª {os.path.basename(file.name)} –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—á–∏—Ç–∞–Ω –∫–∞–∫ —Ç–µ–∫—Å—Ç.")
    
    return file_contents, file_paths

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
def prepare_message_content(message, files):
    content = []
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
    if message:
        content.append({"type": "text", "text": message})
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    file_contents = []
    if files:
        file_contents, file_paths = process_files(files)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        for file_path in file_paths:
            if str(file_path).lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.webp')):
                try:
                    with open(file_path, 'rb') as img_file:
                        base64_image = base64.b64encode(img_file.read()).decode('utf-8')
                        ext = os.path.splitext(file_path)[1][1:]  # –ü–æ–ª—É—á–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –±–µ–∑ —Ç–æ—á–∫–∏
                        content.append({
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/{ext};base64,{base64_image}"
                            }
                        })
                except Exception as e:
                    if message:
                        content[0]["text"] += f"\n\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {os.path.basename(file_path)}: {str(e)}"
                    else:
                        content.append({"type": "text", "text": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {os.path.basename(file_path)}: {str(e)}"})
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫ —Å–æ–æ–±—â–µ–Ω–∏—é
    if file_contents:
        if message:
            content[0]["text"] += "\n\n" + "\n\n".join(file_contents)
        else:
            content.append({"type": "text", "text": "\n\n".join(file_contents)})
    
    # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –ø—É—Å—Ç–æ–π, –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    if not content:
        content.append({"type": "text", "text": ""})
    
    return content

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
active_requests = {}

def process_message(message, history, files, model_name, temperature, max_tokens, system_message):
    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    request_id = str(uuid.uuid4())
    active_requests[request_id] = True
    
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
        messages = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
        if system_message:
            messages.append({"role": "system", "content": system_message})
        
        for user_msg, assistant_msg in history:
            messages.append({"role": "user", "content": user_msg})
            if assistant_msg:
                messages.append({"role": "assistant", "content": assistant_msg})
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        content = prepare_message_content(message, files)
        messages.append({"role": "user", "content": content})
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        request_data = {
            "model": model_name,
            "messages": messages,
            "temperature": float(temperature),
            "stream": True
        }
        
        if max_tokens:
            request_data["max_tokens"] = int(max_tokens)
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ API —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º requests
        response = requests.post(
            "https://glama.ai/api/gateway/openai/v1/chat/completions",
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            },
            json=request_data,
            stream=True
        )
        
        response_text = ""
        for line in response.iter_lines():
            if not active_requests.get(request_id, False):
                # –ó–∞–ø—Ä–æ—Å –±—ã–ª –æ—Ç–º–µ–Ω–µ–Ω
                yield "[–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞]"
                return
            
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    data = line[6:]
                    if data == '[DONE]':
                        break
                    try:
                        chunk = json.loads(data)
                        if chunk.get('choices') and chunk['choices'][0].get('delta') and chunk['choices'][0]['delta'].get('content'):
                            content = chunk['choices'][0]['delta']['content']
                            response_text += content
                            yield response_text
                    except json.JSONDecodeError:
                        pass
        
        return response_text
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ API: {str(e)}"
    finally:
        # –£–¥–∞–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö
        if request_id in active_requests:
            del active_requests[request_id]

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–º–µ–Ω—ã —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
def cancel_generation():
    for request_id in list(active_requests.keys()):
        active_requests[request_id] = False
    return "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
def cleanup_temp_files():
    try:
        for file in TEMP_DIR.glob("*"):
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã —Å—Ç–∞—Ä—à–µ 1 —á–∞—Å–∞
            if time.time() - file.stat().st_mtime > 3600:
                file.unlink()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {str(e)}")

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
def start_cleanup_thread():
    def cleanup_worker():
        while True:
            cleanup_temp_files()
            time.sleep(3600)  # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É –∫–∞–∂–¥—ã–π —á–∞—Å
    
    thread = threading.Thread(target=cleanup_worker, daemon=True)
    thread.start()

# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
available_models = get_available_models()
default_model = available_models[0] if available_models else "gpt-4.5-preview-2025-02-27"

# –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
with gr.Blocks(theme=gr.themes.Soft(), css="""
    footer {visibility: hidden}
    .generating {
        animation: pulse 1.5s infinite;
    }
    @keyframes pulse {
        0% {opacity: 1;}
        50% {opacity: 0.5;}
        100% {opacity: 1;}
    }
""") as demo:
    # –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    current_request_id = gr.State(None)
    
    gr.Markdown("# –ß–∞—Ç —Å LLM —á–µ—Ä–µ–∑ Glama Gateway")
    
    with gr.Row():
        with gr.Column(scale=3):
            model_dropdown = gr.Dropdown(
                choices=available_models,
                label="–ú–æ–¥–µ–ª—å",
                value=default_model,
                info="–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —è–∑—ã–∫–æ–≤–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞"
            )
            refresh_models_btn = gr.Button("–û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π", size="sm")
    
    with gr.Accordion("–ù–∞—Å—Ç—Ä–æ–π–∫–∏", open=False):
        system_message = gr.Textbox(
            label="–°–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ",
            placeholder="–í–≤–µ–¥–∏—Ç–µ —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –º–æ–¥–µ–ª–∏...",
            lines=2
        )
        
        with gr.Row():
            with gr.Column():
                temperature_slider = gr.Slider(
                    minimum=0.0, 
                    maximum=2.0, 
                    value=0.7, 
                    step=0.1, 
                    label="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞",
                    info="–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ (0 = –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, 2 = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Å–ª—É—á–∞–π–Ω—ã–µ)"
                )
            with gr.Column():
                max_tokens_slider = gr.Slider(
                    minimum=100, 
                    maximum=8000, 
                    value=4000, 
                    step=100, 
                    label="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤",
                    info="–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏"
                )
    
    chatbot = gr.Chatbot(height=500, show_copy_button=True, avatar_images=["üë§", "ü§ñ"])
    
    with gr.Row():
        with gr.Column(scale=8):
            msg = gr.Textbox(
                show_label=False,
                placeholder="–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ...",
                container=False,
                lines=3
            )
        with gr.Column(scale=1, min_width=50):
            submit_btn = gr.Button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å", variant="primary")
    
    file_upload = gr.File(file_count="multiple", label="–ü—Ä–∏–∫—Ä–µ–ø–∏—Ç—å —Ñ–∞–π–ª—ã", file_types=["image", "text", ".pdf", ".doc", ".docx"])
    
    with gr.Row():
        clear = gr.Button("–û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç")
        cancel_btn = gr.Button("–û—Ç–º–µ–Ω–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é", variant="stop")
        export_btn = gr.Button("–≠–∫—Å–ø–æ—Ä—Ç –∏—Å—Ç–æ—Ä–∏–∏")
    
    # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è
    status_indicator = gr.Markdown("–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
    def user_input(message, chat_history, files, model_name, temperature, max_tokens, system_msg):
        if not message and not files:
            return "", chat_history, None, "–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–ª–∏ –ø—Ä–∏–∫—Ä–µ–ø–∏—Ç–µ —Ñ–∞–π–ª—ã"
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        yield "", chat_history, None, "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è..."
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        chat_history.append((message, None))
        yield "", chat_history, None, "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏
        bot_message = ""
        for response in process_message(message, chat_history[:-1], files, model_name, temperature, max_tokens, system_msg):
            bot_message = response
            chat_history[-1] = (message, bot_message)
            yield "", chat_history, None, "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..." if "[–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞]" not in bot_message else "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞"
        
        return "", chat_history, None, "–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ"
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π
    def refresh_models():
        new_models = get_available_models(force_refresh=True)
        return gr.Dropdown(choices=new_models, value=new_models[0] if new_models else None), "–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –æ–±–Ω–æ–≤–ª–µ–Ω"
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
    def export_history(chat_history):
        if not chat_history:
            return None, "–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ –ø—É—Å—Ç–∞"
        
        try:
            export_text = "# –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞\n\n"
            for user_msg, bot_msg in chat_history:
                export_text += f"## –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:\n{user_msg}\n\n"
                if bot_msg:
                    export_text += f"## –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:\n{bot_msg}\n\n"
            
            timestamp = time.strftime("%Y%m%d-%H%M%S")
            filename = f"chat_history_{timestamp}.md"
            
            with open(filename, "w", encoding="utf-8") as f:
                f.write(export_text)
            
            return filename, "–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞"
        except Exception as e:
            return None, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ: {str(e)}"
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞—Ç–∞
    def save_chat(chat_history):
        if not chat_history:
            return None, "–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ –ø—É—Å—Ç–∞"
        
        try:
            timestamp = time.strftime("%Y%m%d-%H%M%S")
            filename = f"saved_chat_{timestamp}.json"
            
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(chat_history, f, ensure_ascii=False, indent=2)
            
            return filename, "–ß–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω"
        except Exception as e:
            return None, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {str(e)}"
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞—Ç–∞
    def load_chat(file):
        if not file:
            return [], "–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω"
        
        try:
            with open(file.name, "r", encoding="utf-8") as f:
                chat_history = json.load(f)
            
            return chat_history, "–ß–∞—Ç –∑–∞–≥—Ä—É–∂–µ–Ω"
        except Exception as e:
            return [], f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {str(e)}"
    
    # –ü—Ä–∏–≤—è–∑–∫–∞ —Å–æ–±—ã—Ç–∏–π
    msg.submit(user_input, [msg, chatbot, file_upload, model_dropdown, temperature_slider, max_tokens_slider, system_message], 
               [msg, chatbot, file_upload, status_indicator])
    
    submit_btn.click(user_input, [msg, chatbot, file_upload, model_dropdown, temperature_slider, max_tokens_slider, system_message], 
                    [msg, chatbot, file_upload, status_indicator])
    
    clear.click(lambda: ([], None, "–ß–∞—Ç –æ—á–∏—â–µ–Ω"), outputs=[chatbot, file_upload, status_indicator])
    
    cancel_btn.click(cancel_generation, outputs=[status_indicator])
    
    export_btn.click(export_history, inputs=[chatbot], outputs=[gr.File(label="–°–∫–∞—á–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é"), status_indicator])
    
    refresh_models_btn.click(refresh_models, outputs=[model_dropdown, status_indicator])
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–Ω–æ–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞—Ç–∞
    with gr.Row():
        save_chat_btn = gr.Button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —á–∞—Ç")
        load_chat_file = gr.File(label="–ó–∞–≥—Ä—É–∑–∏—Ç—å —á–∞—Ç", file_types=[".json"])
    
    save_chat_btn.click(save_chat, inputs=[chatbot], outputs=[gr.File(label="–°–∫–∞—á–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —á–∞—Ç"), status_indicator])
    load_chat_file.change(load_chat, inputs=[load_chat_file], outputs=[chatbot, status_indicator])

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
start_cleanup_thread()

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 7860))
    demo.queue().launch(server_name="0.0.0.0", server_port=port)
